# -*- coding: utf-8 -*-
"""Copie de Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10iegkMVFvAn38Z4J3ftlk8CYCdAXmxql
"""

!pip install kaggle

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

!unzip imdb-dataset-of-50k-movie-reviews.zip

import pandas as pd
data = pd.read_csv('IMDB Dataset.csv')

data.head()

import pandas as pd
from bs4 import BeautifulSoup
import re

#Il est aussi possible d'utiliser NLTK mais Avec beautiful soup sa ce supprime en une ligne

# Fonction pour nettoyer le texte avec Beautiful Soup
def clean_text(text):

    # Supprimer les balises HTML
    cleaned_text = BeautifulSoup(text, 'html.parser').get_text()

    # Supprimer la ponctuation
    #cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text)

    # Passer en minuscules
    #cleaned_text = cleaned_text.lower()

    # Supprimer les espaces multiples
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)

    return cleaned_text

# Appliquer la fonction de nettoyage aux données d'entraînement et de test
data['review'] = data['review'].apply(clean_text)

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer


#La réduction de vocabulaire est utilisé seulement pour la classification, car dans d'autres éxemple on a besoin d'article


# Lemmatisation
# On transforme les mots en leurs forme canonnique, c'est à dire la racine, comme par exemple "étudiant" => "étudier" (lemme)
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
  #word is any variable
    return ' '.join([lemmatizer.lemmatize(word) for word  in text.split()])

    #For word dans la liste (text.split() = ["pomme", "grenade"])
    #le coté négatif est qu'ensuite la liste n'est pas ordonnée

data['review'] = data['review'].apply(lemmatize_text)

!pip install keras

import matplotlib.pyplot as plt

# Calculer les longueurs des critiques
review_lengths = [len(review.split(" ")) for review in data['review']]

# Tracer l'histogramme
plt.hist(review_lengths, bins=20, edgecolor='black')
plt.title('Distribution des longueurs des critiques')
plt.xlabel('Longueur (nombre de mots)')
plt.ylabel('Nombre de critiques')
plt.show()

import numpy as np

review_lengths = [len(review.split()) for review in data['review']]

print(f"Moyenne : {np.mean(review_lengths):.2f} mots")
print(f"Médiane : {np.median(review_lengths)} mots")
print(f"Écart-type : {np.std(review_lengths):.2f}")
print(f"Minimum : {np.min(review_lengths)} mots")
print(f"Maximum : {np.max(review_lengths)} mots")



from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.preprocessing.sequence import pad_sequences

#X est le vecteur problème et y le vecteur solution

# Import pad_sequences from keras
# ... rest of the code
from sklearn.preprocessing import LabelEncoder
# Tokenisation et padding
#Le Padding fait en sorte que tous les review est la meme taille
max_length = 100
#Calculer en utilisant la visualisation, la taille moyenne de mes token, j'ais utilisé la
tokenizer = Tokenizer(num_word=500)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['review'])

#Supprimer les review dont la taille est supérrieure à 500 mot

sequences = tokenizer.texts_to_sequences(data['review'])
padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
#post touche à la dérnière parti de la séquence, c'est à dire que si j'ais besoin de conserver la première partie j'utilise post, sinon "pre" si j'ais besoin de la fin

# Encodage des labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(data['sentiment'])

# Séparation des données


from sklearn.model_selection import train_test_split

# Séparer les données en ensembles d'entraînement, de validation et de test
X_train, X_val_test, y_train, y_val_test = train_test_split(padded, labels, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)

#le random
 #Commencer par la, pensée au résultat de mes donnée, comment es ce qu'elle doivent étre structuré

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder

# Calculer la longueur de chaque critique
review_lengths = [len(review.split()) for review in data['review']]

# Filtrer les critiques dont la longueur est inférieure à 500 mots
long_reviews = [review for review, length in zip(data['review'], review_lengths) if length >= 500]
long_sentiments = [sentiment for sentiment, length in zip(data['sentiment'], review_lengths) if length >= 500]

# Tokenisation et padding
max_length = 500  # Ajuster max_length si nécessaire
tokenizer = Tokenizer()
tokenizer.fit_on_texts(long_reviews)
sequences = tokenizer.texts_to_sequences(long_reviews)
padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

# Encodage des labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(long_sentiments)

# Séparation des données


from sklearn.model_selection import train_test_split

# Séparer les données en ensembles d'entraînement, de validation et de test
X_train, X_val_test, y_train, y_val_test = train_test_split(padded, labels, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Learning Rate Decay
lr_decay = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)

"""# Modèle LSTM from"""

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout

model_lstm = Sequential([
    #5000 le nombre de répétition d'un mot
    Embedding(input_dim=2000, output_dim=128), #On ajoute pas d'hyperparamétre par ce qu'on déjas fait un padding et donc
    #ouput dim est la sensibilité du mot
    LSTM(100, return_sequences=True),

    LSTM(64),
    #Encodage commence de l'embedding au LSTM

    #Entre la couche d'entrée et de sortie il y a un certain nombre de couche dense
    #Il y a un neurone dans la couche dense dans ce cas la
    Dense(1, activation='sigmoid')  # Sortie binaire pour la classification
])

model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""# Modèle LSTM avec Attention"""

from keras.models import Model
from keras.layers import Input, Embedding, LSTM, Dense, Attention

input_layer = Input(shape=(500,))
embedding_layer = Embedding(input_dim=2000, output_dim=128)(input_layer)
lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)
attention_layer = Attention()([lstm_layer, lstm_layer])
output_layer = Dense(1, activation='sigmoid')(attention_layer)

model_lstm_attention = Model(inputs=input_layer, outputs=output_layer)
model_lstm_attention.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""# Modèle Transformer"""

from transformers import TFBertForSequenceClassification, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model_transformer = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

"""# Entraînement des modèles

trop de mot
"""

model_lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[early_stopping, lr_decay])
model_lstm_attention.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[early_stopping, lr_decay])

# Pour le modèle Transformer, vous devrez d'abord tokeniser les données
train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)
val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True)

model_transformer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_transformer.fit(train_encodings, y_train, validation_data=(val_encodings, y_val), epochs=10, callbacks=[early_stopping, lr_decay])

model_lstm.save_weights('model_lstm.h5')
model_lstm_attention.save_weights('model_lstm_attention.h5')
model_transformer.save_pretrained('model_transformer')

!pip install tensorflow==2.12.0

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, auc

# Prédictions sur les données de test
y_pred_lstm = model_lstm.predict(X_test)
y_pred_lstm_attention = model_lstm_attention.predict(X_test)
y_pred_transformer = model_transformer.predict(X_test_encoded)

# Calcul de la matrice de confusion
conf_mat_lstm = confusion_matrix(y_test, y_pred_lstm.round())
conf_mat_lstm_attention = confusion_matrix(y_test, y_pred_lstm_attention.round())
conf_mat_transformer = confusion_matrix(y_test, y_pred_transformer.round())

# Calcul des métriques de performance
accuracy_lstm = accuracy_score(y_test, y_pred_lstm.round())
precision_lstm = precision_score(y_test, y_pred_lstm.round())
recall_lstm = recall_score(y_test, y_pred_lstm.round())
f1_lstm = f1_score(y_test, y_pred_lstm.round())

accuracy_lstm_attention = accuracy_score(y_test, y_pred_lstm_attention.round())
precision_lstm_attention = precision_score(y_test, y_pred_lstm_attention.round())
recall_lstm_attention = recall_score(y_test, y_pred_lstm_attention.round())
f1_lstm_attention = f1_score(y_test, y_pred_lstm_attention.round())

accuracy_transformer = accuracy_score(y_test, y_pred_transformer.round())
precision_transformer = precision_score(y_test, y_pred_transformer.round())
recall_transformer = recall_score(y_test, y_pred_transformer.round())
f1_transformer = f1_score(y_test, y_pred_transformer.round())

# Affichage des courbes ROC et calcul de l'AUC
fpr_lstm, tpr_lstm, _ = roc_curve(y_test, y_pred_lstm)
roc_auc_lstm = auc(fpr_lstm, tpr_lstm)

fpr_lstm_attention, tpr_lstm_attention, _ = roc_curve(y_test, y_pred_lstm_attention)
roc_auc_lstm_attention = auc(fpr_lstm_attention, tpr_lstm_attention)

fpr_transformer, tpr_transformer, _ = roc_curve(y_test, y_pred_transformer)
roc_auc_transformer = auc(fpr_transformer, tpr_transformer)

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_lstm:.2f})')
plt.plot(fpr_lstm_attention, tpr_lstm_attention, label=f'LSTM avec Attention (AUC = {roc_auc_lstm_attention:.2f})')
plt.plot(fpr_transformer, tpr_transformer, label=f'Transformer (AUC = {roc_auc_transformer:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('Taux de faux positifs')
plt.ylabel('Taux de vrais positifs')
plt.title('Courbes ROC')
plt.legend()
plt.show()